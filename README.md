# Chat with Your Documents

A Retrieval-Augmented Generation (RAG) application that allows users to chat with their own documents using Large Language Models (LLMs) and semantic search.

---

## Overview

This project implements a complete **RAG pipeline** that combines document retrieval with local language model inference.  
Users can ask questions, and the system retrieves relevant document chunks using a vector database, then generates answers **strictly based on the retrieved context**.

The application runs fully locally and does not rely on external APIs.

---

## Features

- Chat with custom documents
- Semantic search using vector embeddings
- FAISS vector store for fast retrieval
- Local LLM inference (LLaMA via llama-cpp / CTransformers)
- Context-aware answers (no hallucinations)
- Source document references
- Streamlit web interface
- Fully offline and privacy-friendly

---

## Architecture

Documents â†’ Chunking â†’ Embeddings â†’ FAISS
                                      â†“
User Question â†’ Retriever â†’ LLM â†’ Answer

---

## Tech Stack

- Python
- LangChain
- FAISS
- Sentence-Transformers
- LLaMA (GGUF format)
- Streamlit

---

## Project Structure

chat-with-your-documents/
â”œâ”€â”€ app.py

â”œâ”€â”€ ingest.py

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ README.md

â”œâ”€â”€ .gitignore

â””â”€â”€ data/

---
# ðŸ’» Authors
  - Abdul Rahman Ahmed 

  - abdulrahmannassar202@gmail.com

# ðŸ“Œ Project link
  - https://github.com/Abdulrahman181/Smart-Document-Assistant

